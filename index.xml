<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>QDC personal website</title>
    <link>https://TopCoderChen.github.io/</link>
    <description>Recent content on QDC personal website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>©QDC GE 2020</copyright>
    <lastBuildDate>Sun, 21 Jun 2020 17:57:35 -0700</lastBuildDate>
    
	<atom:link href="https://TopCoderChen.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Blockchain learning note</title>
      <link>https://TopCoderChen.github.io/blog/blockchain/</link>
      <pubDate>Sun, 21 Jun 2020 17:57:35 -0700</pubDate>
      
      <guid>https://TopCoderChen.github.io/blog/blockchain/</guid>
      <description>北京大学计算机系肖臻 区块链技术与应用 https://www.youtube.com/playlist?list=PLnTPdMjBRmAYehJkVbAXqxO-0cc9ALC6V
密码学原理 数据结构 协议 实现 </description>
    </item>
    
    <item>
      <title>Batch Normalization</title>
      <link>https://TopCoderChen.github.io/post/batch_norm/</link>
      <pubDate>Sun, 21 Jun 2020 16:58:37 -0700</pubDate>
      
      <guid>https://TopCoderChen.github.io/post/batch_norm/</guid>
      <description>https://cs231n.github.io/neural-networks-2/
Batch normalization is normalizing based on each feature dimention (axis = 0) cross all examples in the batch. While layer normalziation is for single training example.
Gradient calculation It&amp;rsquo;s important to draw the computation dependency graph to better calculate the backward gradient.
The gradient calculation can be made simplified by using the graph and combining common terms in the expression.
One good gradient calculation post : https://kevinzakka.github.io/2016/09/14/batch_normalization/
Fully connected network impl Number of layers = 1 + number of hidden layers</description>
    </item>
    
    <item>
      <title>DDIA reading note</title>
      <link>https://TopCoderChen.github.io/blog/ddia/</link>
      <pubDate>Sun, 21 Jun 2020 15:18:25 -0700</pubDate>
      
      <guid>https://TopCoderChen.github.io/blog/ddia/</guid>
      <description>&amp;laquo;Design Data Intensive Application&amp;raquo;</description>
    </item>
    
    <item>
      <title>CS231n learning note</title>
      <link>https://TopCoderChen.github.io/post/cs231n/</link>
      <pubDate>Sun, 21 Jun 2020 15:02:44 -0700</pubDate>
      
      <guid>https://TopCoderChen.github.io/post/cs231n/</guid>
      <description>Dropout Normally, it&amp;rsquo;s applied after the activation layer.
Inverted dropout is a good variant of dropout.</description>
    </item>
    
  </channel>
</rss>