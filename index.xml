<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>QDC personal website</title>
    <link>https://TopCoderChen.github.io/</link>
    <description>Recent content on QDC personal website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>©QDC GE 2020</copyright>
    <lastBuildDate>Sat, 04 Jul 2020 12:11:58 -0700</lastBuildDate>
    
	<atom:link href="https://TopCoderChen.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>CS231n RNN</title>
      <link>https://TopCoderChen.github.io/post/cs231n-rnn/</link>
      <pubDate>Sat, 04 Jul 2020 12:11:58 -0700</pubDate>
      
      <guid>https://TopCoderChen.github.io/post/cs231n-rnn/</guid>
      <description>Vanila RNN For each timestep, forward pass: $$h_t=f_W(h_{t-1}, x_t)$$ 具体的展开W $$h_t=tanh(W_{hh} * h_{t-1} + W_{xh} * x_t)$$ $$y_t=W_{hy} * h_t$$
在forward中的每一个timestep里return cache，这样就可以在同一个function里拿到cache，然后查看cache的信息，进行backward.
在rnn_backward时，注意gradients of hidden states的来源有两个（需要加起来）:
 dx = np.zeros((N, T, D)) dprev_h = np.zeros((N, H)) dWx = np.zeros((D, H)) dWh = np.zeros((H, H)) db = np.zeros((H)) for t in range(T-1, -1, -1): # from T-1 to 0 dx[:,t,:], dprev_h, dWx_, dWh_, db_ = rnn_step_backward(dh[:,t,:] + dprev_h, cache[t]) dWx += dWx_ dWh += dWh_ db += db_ dh0 = dprev_h CaptioningRNN forward pass  affine_forward() &amp;amp; word_embedding_forward() 分别给出 initial hidden states &amp;amp; captions embedded input, 注意这两个function是独立的:  h0, features_cache = affine_forward(image_features, W_proj, b_proj)</description>
    </item>
    
    <item>
      <title>Convolutional Networks</title>
      <link>https://TopCoderChen.github.io/post/cnn/</link>
      <pubDate>Sat, 27 Jun 2020 11:25:58 -0700</pubDate>
      
      <guid>https://TopCoderChen.github.io/post/cnn/</guid>
      <description>https://cs231n.github.io/assignments2020/assignment2/#q4-convolutional-networks-30-points
Convolution layer implementation In CNN, weight is for filter;
The number of input examples and number of filters all impact how many weight params are needed.
forward pass Note
 Pad the input Don&amp;rsquo;t forget to add bias term  def conv_forward_naive(x, w, b, conv_param): &amp;quot;&amp;quot;&amp;quot; A naive implementation of the forward pass for a convolutional layer. The input consists of N data points, each with C channels, height H and width W.</description>
    </item>
    
    <item>
      <title>Blockchain learning note</title>
      <link>https://TopCoderChen.github.io/blog/blockchain/</link>
      <pubDate>Sun, 21 Jun 2020 17:57:35 -0700</pubDate>
      
      <guid>https://TopCoderChen.github.io/blog/blockchain/</guid>
      <description>北京大学计算机系肖臻
区块链技术与应用
https://www.youtube.com/playlist?list=PLnTPdMjBRmAYehJkVbAXqxO-0cc9ALC6V
肖老师讲的很不错,比较清楚.
密码学原理 数据结构 协议 实现 </description>
    </item>
    
    <item>
      <title>Batch Normalization</title>
      <link>https://TopCoderChen.github.io/post/batch_norm/</link>
      <pubDate>Sun, 21 Jun 2020 16:58:37 -0700</pubDate>
      
      <guid>https://TopCoderChen.github.io/post/batch_norm/</guid>
      <description>https://cs231n.github.io/neural-networks-2/
Batch normalization is normalizing based on each feature dimention (axis = 0) cross all examples in the batch. While layer normalziation is for single training example.
Gradient calculation It&amp;rsquo;s important to draw the computation dependency graph to better calculate the backward gradient.
The gradient calculation can be made simplified by using the graph and combining common terms in the expression.
One good gradient calculation post : https://kevinzakka.github.io/2016/09/14/batch_normalization/
Fully connected network impl Number of layers = 1 + number of hidden layers</description>
    </item>
    
    <item>
      <title>DDIA reading note</title>
      <link>https://TopCoderChen.github.io/blog/ddia/</link>
      <pubDate>Sun, 21 Jun 2020 15:18:25 -0700</pubDate>
      
      <guid>https://TopCoderChen.github.io/blog/ddia/</guid>
      <description>Design Data Intensive Application </description>
    </item>
    
    <item>
      <title>CS231n learning note</title>
      <link>https://TopCoderChen.github.io/post/cs231n/</link>
      <pubDate>Sun, 21 Jun 2020 15:02:44 -0700</pubDate>
      
      <guid>https://TopCoderChen.github.io/post/cs231n/</guid>
      <description>Dropout Normally, it&amp;rsquo;s applied after the activation layer.
Inverted dropout is a good variant of dropout.</description>
    </item>
    
  </channel>
</rss>