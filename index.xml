<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>QDC personal website</title>
    <link>https://TopCoderChen.github.io/</link>
    <description>Recent content on QDC personal website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>©QDC GE 2020</copyright>
    <lastBuildDate>Sat, 27 Jun 2020 11:25:58 -0700</lastBuildDate>
    
	<atom:link href="https://TopCoderChen.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Convolutional Networks</title>
      <link>https://TopCoderChen.github.io/post/cnn/</link>
      <pubDate>Sat, 27 Jun 2020 11:25:58 -0700</pubDate>
      
      <guid>https://TopCoderChen.github.io/post/cnn/</guid>
      <description>https://cs231n.github.io/assignments2020/assignment2/#q4-convolutional-networks-30-points
Convolution layer implementation In CNN, weight is for filter;
The number of input examples and number of filters all impact how many weight params are needed.
forward pass Note
 Pad the input Don&amp;rsquo;t forget to add bias term  def conv_forward_naive(x, w, b, conv_param): &amp;quot;&amp;quot;&amp;quot; A naive implementation of the forward pass for a convolutional layer. The input consists of N data points, each with C channels, height H and width W.</description>
    </item>
    
    <item>
      <title>Blockchain learning note</title>
      <link>https://TopCoderChen.github.io/blog/blockchain/</link>
      <pubDate>Sun, 21 Jun 2020 17:57:35 -0700</pubDate>
      
      <guid>https://TopCoderChen.github.io/blog/blockchain/</guid>
      <description>北京大学计算机系肖臻
区块链技术与应用
https://www.youtube.com/playlist?list=PLnTPdMjBRmAYehJkVbAXqxO-0cc9ALC6V
肖老师讲的很不错,比较清楚.
密码学原理 数据结构 协议 实现 </description>
    </item>
    
    <item>
      <title>Batch Normalization</title>
      <link>https://TopCoderChen.github.io/post/batch_norm/</link>
      <pubDate>Sun, 21 Jun 2020 16:58:37 -0700</pubDate>
      
      <guid>https://TopCoderChen.github.io/post/batch_norm/</guid>
      <description>https://cs231n.github.io/neural-networks-2/
Batch normalization is normalizing based on each feature dimention (axis = 0) cross all examples in the batch. While layer normalziation is for single training example.
Gradient calculation It&amp;rsquo;s important to draw the computation dependency graph to better calculate the backward gradient.
The gradient calculation can be made simplified by using the graph and combining common terms in the expression.
One good gradient calculation post : https://kevinzakka.github.io/2016/09/14/batch_normalization/
Fully connected network impl Number of layers = 1 + number of hidden layers</description>
    </item>
    
    <item>
      <title>DDIA reading note</title>
      <link>https://TopCoderChen.github.io/blog/ddia/</link>
      <pubDate>Sun, 21 Jun 2020 15:18:25 -0700</pubDate>
      
      <guid>https://TopCoderChen.github.io/blog/ddia/</guid>
      <description>Design Data Intensive Application </description>
    </item>
    
    <item>
      <title>CS231n learning note</title>
      <link>https://TopCoderChen.github.io/post/cs231n/</link>
      <pubDate>Sun, 21 Jun 2020 15:02:44 -0700</pubDate>
      
      <guid>https://TopCoderChen.github.io/post/cs231n/</guid>
      <description>Dropout Normally, it&amp;rsquo;s applied after the activation layer.
Inverted dropout is a good variant of dropout.</description>
    </item>
    
  </channel>
</rss>