<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on QDC personal website</title>
    <link>https://TopCoderChen.github.io/post/</link>
    <description>Recent content in Posts on QDC personal website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Â©QDC GE 2020</copyright>
    <lastBuildDate>Sat, 27 Jun 2020 11:25:58 -0700</lastBuildDate>
    
	<atom:link href="https://TopCoderChen.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Convolutional Networks</title>
      <link>https://TopCoderChen.github.io/post/cnn/</link>
      <pubDate>Sat, 27 Jun 2020 11:25:58 -0700</pubDate>
      
      <guid>https://TopCoderChen.github.io/post/cnn/</guid>
      <description>https://cs231n.github.io/assignments2020/assignment2/#q4-convolutional-networks-30-points
Convolution layer implementation In CNN, weight is for filter;
The number of input examples and number of filters all impact how many weight params are needed.
forward pass Note
 Pad the input Don&amp;rsquo;t forget to add bias term  def conv_forward_naive(x, w, b, conv_param): &amp;quot;&amp;quot;&amp;quot; A naive implementation of the forward pass for a convolutional layer. The input consists of N data points, each with C channels, height H and width W.</description>
    </item>
    
    <item>
      <title>Batch Normalization</title>
      <link>https://TopCoderChen.github.io/post/batch_norm/</link>
      <pubDate>Sun, 21 Jun 2020 16:58:37 -0700</pubDate>
      
      <guid>https://TopCoderChen.github.io/post/batch_norm/</guid>
      <description>https://cs231n.github.io/neural-networks-2/
Batch normalization is normalizing based on each feature dimention (axis = 0) cross all examples in the batch. While layer normalziation is for single training example.
Gradient calculation It&amp;rsquo;s important to draw the computation dependency graph to better calculate the backward gradient.
The gradient calculation can be made simplified by using the graph and combining common terms in the expression.
One good gradient calculation post : https://kevinzakka.github.io/2016/09/14/batch_normalization/
Fully connected network impl Number of layers = 1 + number of hidden layers</description>
    </item>
    
    <item>
      <title>CS231n learning note</title>
      <link>https://TopCoderChen.github.io/post/cs231n/</link>
      <pubDate>Sun, 21 Jun 2020 15:02:44 -0700</pubDate>
      
      <guid>https://TopCoderChen.github.io/post/cs231n/</guid>
      <description>Dropout Normally, it&amp;rsquo;s applied after the activation layer.
Inverted dropout is a good variant of dropout.</description>
    </item>
    
  </channel>
</rss>